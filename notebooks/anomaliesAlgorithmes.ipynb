{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef93988-73d7-4693-80ab-6540a2c743b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d05fb4-ea22-4589-bf7a-e71366caffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a3f8ff-0463-406c-b35e-e4e95e5e68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_and_label_anomalies(data: pd.DataFrame, target_anomaly_rate: float = 0.07):\n",
    "    \"\"\"\n",
    "    Inject synthetic anomalies and label natural and synthetic anomalies in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing 'inside_temperature' and 'inside_humidity'.\n",
    "        target_anomaly_rate (float): The target percentage of synthetic anomalies to inject.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Modified dataset with anomalies, DataFrame with anomaly labels.\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Calculate dynamic thresholds using percentiles\n",
    "    lower_temp_threshold = np.percentile(data['inside_temperature'], 2)\n",
    "    upper_temp_threshold = np.percentile(data['inside_temperature'], 55)\n",
    "    lower_hum_threshold = np.percentile(data['inside_humidity'], 30)\n",
    "    upper_hum_threshold = np.percentile(data['inside_humidity'], 80)\n",
    "\n",
    "    # Identify natural anomalies\n",
    "    labels = pd.DataFrame({'anomaly': 0}, index=data.index)\n",
    "    labels.loc[\n",
    "        (data['inside_temperature'] < lower_temp_threshold) | (data['inside_temperature'] > upper_temp_threshold) |\n",
    "        (data['inside_humidity'] < lower_hum_threshold) | (data['inside_humidity'] > upper_hum_threshold),\n",
    "        'anomaly'\n",
    "    ] = 1\n",
    "\n",
    "    # Inject synthetic anomalies\n",
    "    num_synthetic_anomalies = int(len(data) * target_anomaly_rate)\n",
    "    synthetic_anomaly_indices = np.random.choice(\n",
    "        data.index, size=num_synthetic_anomalies, replace=False\n",
    "    )\n",
    "    data.loc[synthetic_anomaly_indices, 'inside_temperature'] += np.random.uniform(65, 100, size=num_synthetic_anomalies)\n",
    "    data.loc[synthetic_anomaly_indices, 'inside_humidity'] += np.random.uniform(-10, 20, size=num_synthetic_anomalies)\n",
    "\n",
    "    labels.loc[synthetic_anomaly_indices, 'anomaly'] = 1\n",
    "    print(labels.shape)\n",
    "\n",
    "    return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993dd638-cb3f-4356-b07d-050030cb225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocess data with null value handling\n",
    "    \"\"\"\n",
    "    data['time'] = pd.to_datetime(data['time'])\n",
    "\n",
    "    processed_data = data.copy()\n",
    "    \n",
    "    # Drop house column if exists\n",
    "    if 'house' in processed_data.columns:\n",
    "        processed_data = processed_data.drop('house', axis=1)\n",
    "    \n",
    "    # Time features\n",
    "    processed_data['hour'] = processed_data['time'].dt.hour\n",
    "    processed_data['day_of_week'] = processed_data['time'].dt.dayofweek\n",
    "    processed_data['month'] = processed_data['time'].dt.month\n",
    "    processed_data['day'] = processed_data['time'].dt.day\n",
    "    processed_data['is_weekend'] = processed_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Handle missing values\n",
    "    numerical_cols = ['outside_temperature', 'outside_humidity','BP','WS','WD_Avg','WSgust_Max','Rain_mm_Tot',\n",
    "                     'hour', 'day_of_week', 'month', 'day']\n",
    "    \n",
    "    # For numerical columns: interpolate between values, then forward/backward fill any remaining nulls\n",
    "    for col in numerical_cols:\n",
    "        if col in processed_data.columns:\n",
    "            processed_data[col] = (processed_data[col]\n",
    "                                 .interpolate(method='linear')\n",
    "                                 .fillna(method='ffill')\n",
    "                                 .fillna(method='bfill'))\n",
    "    \n",
    "    # For categorical columns: fill with mode\n",
    "    categorical_cols = processed_data.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col != 'time':  # Skip time column\n",
    "            processed_data[col] = processed_data[col].fillna(processed_data[col].mode()[0])\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    processed_data = pd.get_dummies(processed_data, columns=['zone', 'device_id'])\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    processed_data[numerical_cols] = scaler.fit_transform(processed_data[numerical_cols])\n",
    "    \n",
    "    # Verify no nulls remain\n",
    "    assert processed_data.isnull().sum().sum() == 0, \"Null values remain after preprocessing\"\n",
    "    \n",
    "    return processed_data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4edf1b8-fe96-47c3-8069-b9356e44fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_anomalies(data: pd.DataFrame, labels: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualize the anomalies in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing 'inside_temperature' and 'inside_humidity'.\n",
    "        labels (pd.DataFrame): The DataFrame containing anomaly labels (1 for anomalies, 0 for normal points).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Plot temperature\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(data.index, data['inside_temperature'], label='Temperature', alpha=0.3, s=5, color='blue')\n",
    "    \n",
    "    # Highlight anomalies\n",
    "    anomalies = data[labels['anomaly'] == 1]\n",
    "    plt.scatter(anomalies.index, anomalies['inside_temperature'], \n",
    "                color='red', label='Anomalies', edgecolor='black', s=10)\n",
    "\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Temperature')\n",
    "    plt.title('Anomaly Detection in Temperature')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Plot humidity\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(data.index, data['inside_humidity'], label='Humidity', alpha=0.3, s=5, color='green')\n",
    "\n",
    "    # Highlight anomalies\n",
    "    plt.scatter(anomalies.index, anomalies['inside_humidity'], \n",
    "                color='red', label='Anomalies', edgecolor='black', s=10)\n",
    "\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Humidity')\n",
    "    plt.title('Anomaly Detection in Humidity')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "661ec8c5-773c-455f-bb58-bbb82b09ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_xgboost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for XGBoost using GridSearchCV\n",
    "    \"\"\"\n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [200],\n",
    "        'learning_rate': [0.02],\n",
    "        'max_depth': [ 6],\n",
    "        'subsample': [ 1.0],\n",
    "        'colsample_bytree': [ 1.0]\n",
    "    }\n",
    "\n",
    "    # Instantiate the model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    # GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1',  # Or use 'roc_auc', 'accuracy', etc.\n",
    "        cv=2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and best score\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best F1 Score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8926a7-07dc-43fe-92d3-64bd4e5200ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train Naive Bayes classifier\n",
    "    \"\"\"\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99845455-623a-4031-923a-2b2283dcee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Enhanced autoencoder with deeper architecture and dropout\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(256, activation='relu')(input_layer)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    encoded = Dense(128, activation='relu')(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dropout(0.2)(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dense(16, activation='relu')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(32, activation='relu')(encoded)\n",
    "    decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    decoded = Dense(256, activation='relu')(decoded)\n",
    "    decoded = Dropout(0.2)(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a89226ef-8265-4abf-882b-19ea4e7b5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def train_lof_with_tuning(X_train):\n",
    "    \"\"\"\n",
    "    Perform parameter tuning for LOF using GridSearchCV\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "    params = {\n",
    "        'n_neighbors': [ 50, 100],\n",
    "        'contamination': [0.05, 0.1, 0.2],\n",
    "        'metric': ['euclidean']\n",
    "    }\n",
    "\n",
    "    # Initialize LOF model in novelty mode\n",
    "    lof_model = LocalOutlierFactor(novelty=True)\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(estimator=lof_model, param_grid=params, cv=2, scoring='f1', verbose=1)\n",
    "    grid_search.fit(X_train, X_train)  # LOF in novelty mode fits X_train to itself\n",
    "\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best F1 Score:\", grid_search.best_score_)\n",
    "\n",
    "    # Return the best model\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e5cd04-085c-4467-afa5-776371b8683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(y_test, predictions_dict):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for all models\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for model_name, y_pred_proba in predictions_dict.items():\n",
    "        if y_pred_proba is not None:  # Some models might not have probability predictions\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Different Models')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca764616-249a-4020-bcb1-ec35195cf37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_autoencoder_tuning(input_dim, encoding_dims, dropout_rate, learning_rate, activation):\n",
    "    \"\"\"\n",
    "    Parametrized autoencoder creation function for tuning\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    encoded = Dense(encoding_dims[0], activation=activation)(input_layer)\n",
    "    encoded = Dropout(dropout_rate)(encoded)\n",
    "    for dim in encoding_dims[1:-1]:\n",
    "        encoded = Dense(dim, activation=activation)(encoded)\n",
    "        encoded = Dropout(dropout_rate)(encoded)\n",
    "        encoded = BatchNormalization()(encoded)\n",
    "    encoded = Dense(encoding_dims[-1], activation=activation)(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = encoded\n",
    "    for dim in reversed(encoding_dims[:-1]):\n",
    "        decoded = Dense(dim, activation=activation)(decoded)\n",
    "        decoded = Dropout(dropout_rate)(decoded)\n",
    "        decoded = BatchNormalization()(decoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)  # Output activation should match the problem\n",
    "    \n",
    "    # Autoencoder\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "def train_and_evaluate_autoencoder(X_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning and train the final model\n",
    "    \"\"\"\n",
    "    # Define parameter space\n",
    "    param_grid = {\n",
    "    'model__encoding_dims': [[256, 128, 64, 32, 16]],\n",
    "    'model__dropout_rate': [0.1],\n",
    "    'model__learning_rate': [0.001],\n",
    "    'model__activation': ['relu'],\n",
    "    'batch_size': [128],\n",
    "    'epochs': [50]\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Create model for tuning with scikeras\n",
    "    model = KerasRegressor(\n",
    "        model=create_autoencoder_tuning,  \n",
    "        input_dim=X_train.shape[1],       \n",
    "        encoding_dims=[256, 128, 64, 32, 16], \n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001,\n",
    "        activation='relu',                \n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Random search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=1,\n",
    "        cv=2,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=1, \n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    random_search.fit(X_train, X_train)\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    # Extract model parameters from best_params\n",
    "    model_params = {k.replace('model__', ''): v for k, v in best_params.items() \n",
    "                   if k.startswith('model__')}\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    final_model = create_autoencoder_tuning(\n",
    "        X_train.shape[1],\n",
    "        **model_params\n",
    "    )\n",
    "    \n",
    "    history = final_model.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=best_params['epochs'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    reconstructed = final_model.predict(X_test)\n",
    "    mse = np.mean(np.power(X_test - reconstructed, 2), axis=1)\n",
    "    threshold = np.percentile(mse, 90)\n",
    "    ae_pred = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate and return results\n",
    "    metrics = evaluate_model(y_test, ae_pred, mse, \"Tuned Autoencoder\")\n",
    "    return final_model, mse, metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd27623b-b567-425e-a71d-6282a1c8c743",
   "metadata": {},
   "source": [
    "!pip install scikeras"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ba6adf8-574d-4b59-99c0-2c8bfe02d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    print('y_true',y_true.shape)\n",
    "    print('y_pred',y_pred.shape)\n",
    "    print('y_pred_proba',y_pred_proba.shape)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return precision, recall, f1, y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8c0caec-a769-4df5-b6b7-0b7f2fb6420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndDetect(data):\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"Starting anomaly detection analysis...\")\n",
    "    \n",
    "    # Generate anomaly labels\n",
    "    print(\"\\nGenerating anomaly labels...\")\n",
    "    data, y = inject_and_label_anomalies(data)\n",
    "    \n",
    "    # Visualize the anomalies\n",
    "    print(\"\\nVisualizing anomalies...\")\n",
    "    visualize_anomalies(data, y)\n",
    "    \n",
    "    # Preprocess data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    processed_data, scaler = preprocess_data(data)\n",
    "    \n",
    "    # Prepare features\n",
    "    X = processed_data.drop(['time'], axis=1)\n",
    "    \n",
    "    # Train-test split\n",
    "    print(\"\\nSplitting data into train and test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    \n",
    "    # Dictionary to store predictions for ROC curves\n",
    "    predictions_dict = {}\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    \n",
    "    # 1. XGBoost\n",
    "    print(\"\\nTraining XGBoost...\")\n",
    "    print(\"\\nTuning XGBoost...\")\n",
    "    tuned_xgb_model = tune_xgboost(X_train, y_train)\n",
    "    xgb_pred = tuned_xgb_model.predict(X_test)\n",
    "    xgb_pred_proba = tuned_xgb_model.predict_proba(X_test)[:, 1]\n",
    "    print(xgb_pred)\n",
    "    print(y_test)\n",
    "    xgb_metrics = evaluate_model(y_test, xgb_pred, xgb_pred_proba, \"Tuned XGBoost\")\n",
    "    predictions_dict['Tuned XGBoost'] = xgb_pred_proba\n",
    "\n",
    "    \n",
    "    # 2. Naive Bayes\n",
    "    print(\"\\nTraining Naive Bayes...\")\n",
    "    nb_model = train_naive_bayes(X_train, y_train)\n",
    "    nb_pred = nb_model.predict(X_test)\n",
    "    nb_pred_proba = nb_model.predict_proba(X_test)[:, 1]\n",
    "    nb_metrics = evaluate_model(y_test, nb_pred, nb_pred_proba, \"Naive Bayes\")\n",
    "    predictions_dict['Naive Bayes'] = nb_pred_proba\n",
    "    \n",
    "    # 3. Autoencoder\n",
    "    #print(\"\\nTraining Autoencoder...\") \n",
    "    #autoencoder = create_autoencoder(X_train.shape[1])\n",
    "    #autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    # Get reconstruction error\n",
    "    #reconstructed = autoencoder.predict(X_test)\n",
    "    #mse = np.mean(np.power(X_test - reconstructed, 2), axis=1)\n",
    "    #threshold = np.percentile(mse, 90)\n",
    "    #ae_pred = (mse > threshold).astype(int)\n",
    "    #ae_metrics = evaluate_model(y_test, ae_pred, mse, \"Autoencoder\")\n",
    "    #predictions_dict['Autoencoder'] = mse\n",
    "\n",
    "    # 3. Autoencoder parameters tuning\n",
    "    print(\"\\nStarting autoencoder parameter tuning and training...\")\n",
    "    final_autoencoder, mse_scores, (precision, recall, f1, _) = train_and_evaluate_autoencoder(X_train, X_test, y_test)\n",
    "    predictions_dict['Tuned_Autoencoder'] = mse_scores\n",
    "    \n",
    "    # 4. LOF\n",
    "    print(\"\\nTraining Local Outlier Factor with Tuning...\")\n",
    "    lof_model = train_lof_with_tuning(X_train)\n",
    "    lof_pred = lof_model.predict(X_test)\n",
    "    lof_pred = (lof_pred == -1).astype(int)\n",
    "    lof_scores = -lof_model.score_samples(X_test)\n",
    "    lof_metrics = evaluate_model(y_test, lof_pred, lof_scores, \"LOF with Tuning\")\n",
    "    predictions_dict['LOF_Tuned'] = lof_scores\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    print(\"\\nPlotting ROC curves...\")\n",
    "    plot_roc_curves(y_test, predictions_dict)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    models = ['XGBoost', 'Naive Bayes', 'Autoencoder', 'LOF']\n",
    "    metrics = [xgb_metrics[:3], nb_metrics[:3], ae_metrics[:3], lof_metrics[:3]]\n",
    "    \n",
    "    comparison_df = pd.DataFrame(metrics, \n",
    "                               columns=['Precision', 'Recall', 'F1'],\n",
    "                               index=models)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10,6))\n",
    "    comparison_df.plot(kind='bar')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df, {\n",
    "        'xgboost': xgb_model,\n",
    "        'naive_bayes': nb_model,\n",
    "        'autoencoder': autoencoder,\n",
    "        'lof': lof_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "id": "90071f3e-8d09-478c-96cf-3e9e77ba64df",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "file_path = \"../data/processed/imputedData.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "results, models = trainAndDetect(data)\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d7f99aa548056fcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6296f9-10ab-4b14-8231-d5c2168a6192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
